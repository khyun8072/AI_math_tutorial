{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khyun8072/AI_math_tutorial/blob/main/Week3/251021_%E1%84%80%E1%85%AE%E1%86%ABAI_%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8%E1%84%8C%E1%85%A1%E1%84%85%E1%85%AD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyaraigXphqY"
      },
      "source": [
        "# EN→FR Translation: LSTM+Attention vs From-Scratch Transformer vs Fine-tuned T5\n",
        "\n",
        "이 노트북은 **영어→프랑스어 기계번역**을 위한 세 가지 딥러닝 접근법을 비교합니다.\n",
        "\n",
        "## 다루는 내용\n",
        "\n",
        "### 데이터셋\n",
        "- **Dataset:** `Helsinki-NLP/opus-100` (영–독 병렬 말뭉치)\n",
        "- 고품질 병렬 번역 데이터로 약 100만 쌍의 영어-프랑스어 문장 포함\n",
        "\n",
        "### 비교 모델\n",
        "- **Model A (LSTM+Attention)**:\n",
        "  - Seq2Seq 아키텍처 기반 LSTM 인코더-디코더\n",
        "  - Bahdanau Attention 메커니즘으로 긴 문장 처리 개선\n",
        "  - 전통적이지만 효과적인 RNN 기반 번역 모델\n",
        "  \n",
        "- **Model B (From-scratch Transformer)**:\n",
        "  - 무작위 초기화된 소형 Transformer (BERT tokenizer 사용)\n",
        "  - 완전히 새롭게 학습하는 인코더-디코더 구조\n",
        "  - Self-attention으로 병렬 처리와 장거리 의존성 포착\n",
        "  \n",
        "- **Model C (Pretrained T5 fine-tuning)**:\n",
        "  - `t5-small`을 영→프 번역 태스크에 미세조정\n",
        "  - 대규모 사전학습의 이점 활용\n",
        "  - Text-to-Text 프레임워크로 태스크 통일\n",
        "\n",
        "### 평가 & 해석\n",
        "- **평가 지표**: `sacreBLEU` - 표준 기계번역 평가 메트릭\n",
        "- **시각화**: **Attention Map**으로 각 모델의 내부 동작 비교\n",
        "- **질적 분석**: 동일 문장에 대한 세 모델의 번역 품질 비교\n",
        "\n",
        "## 학습 목표\n",
        "1. RNN 기반(LSTM)과 Transformer 기반 아키텍처의 차이 이해\n",
        "2. 사전학습 모델과 From-scratch 학습의 성능 차이 체험\n",
        "3. Attention 메커니즘의 작동 원리를 시각화로 직관적 이해\n",
        "4. 각 접근법의 장단점과 실무 적용 시나리오 파악"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScpkgHlgphqZ"
      },
      "source": [
        "## 0) 환경 설정\n",
        "\n",
        "필요한 라이브러리를 설치합니다. 이 셀은 Colab 또는 새로운 환경에서 처음 실행할 때만 필요합니다.\n",
        "\n",
        "### 설치되는 패키지:\n",
        "- `transformers`: Hugging Face의 사전학습 모델 및 토크나이저\n",
        "- `datasets`: 데이터셋 로드 및 전처리\n",
        "- `accelerate`: 학습 가속화 도구\n",
        "- `evaluate`: 평가 메트릭 라이브러리\n",
        "- `sacrebleu`: BLEU 점수 계산\n",
        "- `sentencepiece`: T5 토크나이저용 서브워드 분할\n",
        "- `matplotlib`: 시각화\n",
        "- `torch`: PyTorch 딥러닝 프레임워크"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEnrJ-RJphqZ"
      },
      "outputs": [],
      "source": [
        "!pip -q install evaluate sacrebleu sentencepiece --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds0MYAPQphqZ"
      },
      "source": [
        "## 1) 라이브러리 임포트 & 시드 설정\n",
        "\n",
        "이 섹션에서는 필요한 모든 라이브러리를 임포트하고 재현성을 위한 랜덤 시드를 설정합니다.\n",
        "\n",
        "### 재현성(Reproducibility)의 중요성:\n",
        "- 동일한 시드를 사용하면 매번 같은 결과를 얻을 수 있습니다\n",
        "- 실험 결과를 비교하고 디버깅할 때 필수적입니다\n",
        "- Python, NumPy, PyTorch의 모든 난수 생성기를 고정합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxSg-mBCphqZ"
      },
      "outputs": [],
      "source": [
        "import os, math, random, torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import evaluate\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq, Trainer, TrainingArguments,\n",
        "    EncoderDecoderModel, BertConfig\n",
        ")\n",
        "\n",
        "# 재현성을 위한 모든 랜덤 시드 고정\n",
        "SEED = 42\n",
        "random.seed(SEED)  # Python 랜덤 시드\n",
        "np.random.seed(SEED)  # NumPy 랜덤 시드\n",
        "torch.manual_seed(SEED)  # PyTorch CPU 랜덤 시드\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)  # PyTorch GPU 랜덤 시드\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # 사용할 GPU 설정\n",
        "os.environ[\"HF_HUB_ENABLE_XET\"] = \"0\"\n",
        "# GPU 사용 가능 여부 확인 및 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7hbX5u9phqa"
      },
      "source": [
        "## 2) 데이터셋 로드 (`Helsinki-NLP/opus-100`, EN→FR)\n",
        "\n",
        "### OPUS-100 데이터셋이란?\n",
        "- **OPUS**는 공개 병렬 말뭉치(Open Parallel Corpus) 프로젝트입니다\n",
        "- **100개 언어 쌍**을 포함하는 대규모 다국어 번역 데이터셋\n",
        "- 영어-프랑스어(EN-FR) 쌍은 고품질 번역 데이터로 구성\n",
        "\n",
        "### 데이터 구조:\n",
        "- `train`: 학습용 데이터 (약 100만 문장 쌍)\n",
        "- `validation`: 검증용 데이터 (모델 성능 모니터링)\n",
        "- `test`: 최종 평가용 데이터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLEwHb7Wphqa"
      },
      "outputs": [],
      "source": [
        "# 언어 쌍 설정: 영어(en) → 프랑스어(fr)\n",
        "lang_pair = (\"en\", \"fr\")\n",
        "\n",
        "# Hugging Face Hub에서 OPUS-100 데이터셋 다운로드\n",
        "# 첫 실행 시 자동으로 다운로드되며, 이후에는 캐시된 데이터 사용\n",
        "dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-fr\")\n",
        "\n",
        "# 데이터 크기 확인\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_eEhg80phqa"
      },
      "source": [
        "### 빠른 실습을 위한 서브셋\n",
        "\n",
        "전체 데이터셋은 학습 시간이 오래 걸리므로, 빠른 실험을 위해 서브셋을 사용합니다.\n",
        "\n",
        "**서브셋 크기:**\n",
        "- 학습(train): 10,000 문장 쌍\n",
        "- 검증(validation): 1,000 문장 쌍  \n",
        "- 테스트(test): 1,000 문장 쌍\n",
        "\n",
        "**참고:** 실제 프로덕션 모델 학습 시에는 `USE_SUBSET = False`로 설정하여 전체 데이터를 사용하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB6Zzftephqa"
      },
      "outputs": [],
      "source": [
        "# 서브셋 사용 여부 설정\n",
        "USE_SUBSET = True\n",
        "TRAIN_SAMPLES = 10_000  # 학습 데이터 개수\n",
        "VAL_SAMPLES   = 1_000   # 검증 데이터 개수\n",
        "TEST_SAMPLES  = 1_000   # 테스트 데이터 개수\n",
        "\n",
        "def take_subset(ds, n):\n",
        "    \"\"\"\n",
        "    데이터셋에서 처음 n개의 샘플만 추출하는 함수\n",
        "\n",
        "    Args:\n",
        "        ds: Hugging Face Dataset 객체\n",
        "        n: 추출할 샘플 개수\n",
        "\n",
        "    Returns:\n",
        "        n개의 샘플로 이루어진 새로운 Dataset 객체\n",
        "    \"\"\"\n",
        "    return ds.select(range(min(n, len(ds))))\n",
        "\n",
        "# 서브셋 생성 또는 전체 데이터셋 사용\n",
        "if USE_SUBSET:\n",
        "    dataset_small = DatasetDict({\n",
        "        \"train\": take_subset(dataset[\"train\"], TRAIN_SAMPLES),\n",
        "        \"validation\": take_subset(dataset[\"validation\"], VAL_SAMPLES),\n",
        "        \"test\": take_subset(dataset[\"test\"], TEST_SAMPLES),\n",
        "    })\n",
        "else:\n",
        "    dataset_small = dataset\n",
        "\n",
        "# 각 split의 크기 출력\n",
        "print(\"Dataset splits:\", {k: len(v) for k, v in dataset_small.items()})\n",
        "dataset_small"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X77ceAy-phqa"
      },
      "source": [
        "## 3) 토크나이저 설정\n",
        "\n",
        "토크나이저는 텍스트를 모델이 이해할 수 있는 숫자(토큰 ID)로 변환하는 역할을 합니다.\n",
        "\n",
        "### 왜 서로 다른 토크나이저를 사용하나요?\n",
        "\n",
        "각 모델은 서로 다른 토크나이제이션 방식으로 사전학습되었기 때문에, 해당 모델에 맞는 토크나이저를 사용해야 합니다.\n",
        "\n",
        "**1. BERT Multilingual Tokenizer (LSTM + From-scratch Transformer용)**\n",
        "- WordPiece 기반 서브워드 분할\n",
        "- 다국어 지원 (영어, 프랑스어 모두 처리 가능)\n",
        "- 어휘 크기: 약 119,547개\n",
        "\n",
        "**2. T5 Tokenizer (Fine-tuned T5용)**\n",
        "- SentencePiece 기반 서브워드 분할\n",
        "- Task prefix 지원 (\"translate English to French: \")\n",
        "- 어휘 크기: 32,128개"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOW2iWZaphqa"
      },
      "outputs": [],
      "source": [
        "# ==================== BERT Multilingual 토크나이저 설정 ====================\n",
        "# LSTM + From-scratch Transformer에서 사용\n",
        "tok_name = \"google-bert/bert-base-multilingual-cased\"\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(tok_name)\n",
        "\n",
        "# 특수 토큰 ID 저장 (패딩, 문장 시작/끝 등을 나타내는 특별한 토큰)\n",
        "PAD_ID = tokenizer_bert.pad_token_id  # 패딩 토큰: 배치 내 문장 길이를 맞추기 위해 사용\n",
        "UNK_ID = tokenizer_bert.unk_token_id  # 미등록 토큰: 어휘에 없는 단어\n",
        "CLS_ID = tokenizer_bert.cls_token_id  # 문장 시작 토큰\n",
        "SEP_ID = tokenizer_bert.sep_token_id  # 문장 끝 토큰\n",
        "VOCAB_SIZE = tokenizer_bert.vocab_size  # 전체 어휘 크기\n",
        "\n",
        "max_length_bert = 128  # BERT 토크나이저의 최대 시퀀스 길이\n",
        "\n",
        "# ==================== T5 토크나이저 설정 ====================\n",
        "# Fine-tuned T5 모델에서 사용\n",
        "model_name_t5 = \"google/flan-t5-small\"\n",
        "tokenizer_t5 = AutoTokenizer.from_pretrained(model_name_t5)\n",
        "\n",
        "# T5는 task prefix를 사용하여 어떤 작업을 수행할지 모델에게 알려줌\n",
        "task_prefix = \"translate English to French: \"\n",
        "\n",
        "max_source_length = 128  # 입력 문장의 최대 길이\n",
        "max_target_length = 128  # 출력 문장의 최대 길이\n",
        "\n",
        "# 토크나이저 정보 출력\n",
        "print(\"BERT Tokenizer:\", tok_name, \"| Vocab size:\", VOCAB_SIZE)\n",
        "print(\"T5 Tokenizer:\", model_name_t5, \"| Vocab size:\", tokenizer_t5.vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-DtWuuaphqa"
      },
      "source": [
        "## 4) 평가 지표: sacreBLEU\n",
        "\n",
        "### BLEU (Bilingual Evaluation Understudy)란?\n",
        "- 기계번역의 품질을 자동으로 평가하는 표준 메트릭\n",
        "- 기계번역 결과와 사람이 번역한 참조 번역을 비교\n",
        "- n-gram 중복도를 기반으로 0-100점 사이의 점수 계산\n",
        "\n",
        "### sacreBLEU\n",
        "- BLEU의 표준화된 구현체\n",
        "- 재현 가능하고 일관된 평가를 보장\n",
        "- 논문에서 보고하는 점수와 비교 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkKKy3Rjphqa"
      },
      "outputs": [],
      "source": [
        "# sacreBLEU 메트릭 로드\n",
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    \"\"\"\n",
        "    예측 결과와 레이블 텍스트의 후처리를 수행하는 함수\n",
        "\n",
        "    역할:\n",
        "    - BLEU 계산을 위해 텍스트 형식을 맞춰줌\n",
        "    - 예측값: 리스트 형태로 유지\n",
        "    - 레이블: 이중 리스트로 변환 (BLEU는 여러 참조 번역을 지원하기 때문)\n",
        "\n",
        "    Args:\n",
        "        preds: 예측된 번역 텍스트 리스트\n",
        "        labels: 정답 번역 텍스트 리스트\n",
        "\n",
        "    Returns:\n",
        "        전처리된 (preds, labels) 튜플\n",
        "    \"\"\"\n",
        "    # 앞뒤 공백 제거\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    # BLEU는 여러 참조 번역을 지원하므로 각 레이블을 리스트로 감싸줌\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics_t5(eval_pred):\n",
        "    \"\"\"\n",
        "    T5 모델의 평가 메트릭을 계산하는 함수\n",
        "\n",
        "    역할:\n",
        "    - Trainer가 검증 단계에서 자동으로 호출\n",
        "    - 토큰 ID를 텍스트로 디코딩\n",
        "    - sacreBLEU 점수 계산\n",
        "\n",
        "    Args:\n",
        "        eval_pred: (predictions, labels) 튜플\n",
        "\n",
        "    Returns:\n",
        "        {\"sacrebleu\": score} 딕셔너리\n",
        "    \"\"\"\n",
        "    preds, labels = eval_pred\n",
        "    # 튜플 형태로 반환되는 경우 첫 번째 요소만 사용\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    # 토큰 ID를 텍스트로 변환\n",
        "    decoded_preds = tokenizer_t5.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # -100은 loss 계산에서 무시되는 토큰이므로 pad_token_id로 대체\n",
        "    labels = np.where(labels != -100, labels, tokenizer_t5.pad_token_id)\n",
        "    decoded_labels = tokenizer_t5.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # 후처리 및 BLEU 점수 계산\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return {\"sacrebleu\": result[\"score\"]}\n",
        "\n",
        "def compute_metrics_bert(eval_pred):\n",
        "    \"\"\"\n",
        "    BERT 기반 모델의 평가 메트릭을 계산하는 함수\n",
        "\n",
        "    역할:\n",
        "    - From-scratch Transformer 모델 평가에 사용\n",
        "    - T5와 동일한 방식이지만 BERT 토크나이저 사용\n",
        "\n",
        "    Args:\n",
        "        eval_pred: (predictions, labels) 튜플\n",
        "\n",
        "    Returns:\n",
        "        {\"sacrebleu\": score} 딕셔너리\n",
        "    \"\"\"\n",
        "    preds, labels = eval_pred\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    # BERT 토크나이저로 디코딩\n",
        "    decoded_preds = tokenizer_bert.batch_decode(preds, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer_bert.pad_token_id)\n",
        "    decoded_labels = tokenizer_bert.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return {\"sacrebleu\": result[\"score\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bCKehMsphqa"
      },
      "source": [
        "---\n",
        "## 5) Model A: LSTM Seq2Seq with Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "152b7fb7"
      },
      "source": [
        "### Attention 메커니즘 한눈에 보기\n",
        "\n",
        "**Attention**은 디코더가 인코더의 어떤 토큰에 집중해야 하는지 학습으로 결정하게 해 주는 가중치 테이블입니다.\n",
        "\n",
        "#### 핵심 개념:\n",
        "1. **컨텍스트 벡터**: 각 디코딩 스텝마다 소프트맥스를 통해 중요도가 계산되며, 그 결과로 컨텍스트 벡터가 만들어집니다.\n",
        "2. **장문 처리**: LSTM과 결합하면 길이가 긴 문장에서도 중요한 단어를 잊지 않고 번역 품질을 끌어올릴 수 있습니다.\n",
        "3. **해석 가능성**: 시각화된 히트맵은 어느 소스 토큰이 대응되는지 직관적으로 보여 줍니다.\n",
        "\n",
        "#### Attention의 동작 방식 (3단계):\n",
        "\n",
        "**1단계: Attention Scores 계산**\n",
        "- **Query (Q)**: 디코더의 현재 hidden state - \"지금 생성할 단어는 무엇인가?\"\n",
        "- **Keys (K)**: 인코더의 모든 hidden states - \"소스 문장의 각 단어 정보\"\n",
        "- **Scores**: Query와 각 Key 간의 유사도 계산 (dot product)\n",
        "  - `score(i) = Query · Key(i)` (스케일링 적용)\n",
        "\n",
        "**2단계: Attention Distribution 생성**\n",
        "- **Attention Weights (α)**: Scores에 softmax를 적용하여 확률 분포로 변환\n",
        "  - `α(i) = softmax(scores) = exp(score(i)) / Σ exp(score(j))`\n",
        "  - 모든 가중치의 합은 1.0 (확률 분포)\n",
        "  - 높은 가중치 = 해당 소스 단어에 더 집중\n",
        "\n",
        "**3단계: Attention Output 생성**\n",
        "- **Values (V)**: 인코더의 모든 hidden states (Keys와 동일)\n",
        "- **Context Vector**: Attention Weights와 Values의 가중합\n",
        "  - `context = Σ α(i) × Value(i)`\n",
        "  - 중요한 소스 단어들의 정보가 강조된 벡터\n",
        "  - 이 벡터가 디코더의 입력으로 사용되어 다음 단어 예측에 활용됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47e23c1c"
      },
      "source": [
        "![image.png](attachment:image.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxuiVNpXphqb"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM 기반 인코더\n",
        "\n",
        "    역할:\n",
        "    - 입력 문장(소스 언어)을 임베딩 벡터로 변환\n",
        "    - LSTM을 통해 시퀀스를 인코딩하여 hidden states 생성\n",
        "    - 각 타임스텝의 출력과 최종 hidden/cell state 반환\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # 단어 ID를 dense vector로 변환하는 임베딩 레이어\n",
        "        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_ID)\n",
        "        # 양방향이 아닌 단방향 LSTM (Seq2Seq에서는 단방향 사용)\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True,\n",
        "                           dropout=dropout if num_layers>1 else 0.0)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: [batch_size, src_len] → x: [batch_size, src_len, embed_dim]\n",
        "        x = self.emb(src)\n",
        "        # outputs: [batch_size, src_len, hidden_dim] - 각 타임스텝의 출력\n",
        "        # h: [num_layers, batch_size, hidden_dim] - 최종 hidden state\n",
        "        # c: [num_layers, batch_size, hidden_dim] - 최종 cell state\n",
        "        outputs, (h, c) = self.rnn(x)\n",
        "        return outputs, (h, c)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Bahdanau Attention 메커니즘\n",
        "\n",
        "    역할:\n",
        "    - 디코더의 현재 상태와 인코더의 모든 출력 간의 유사도 계산\n",
        "    - 소프트맥스로 attention weights 생성\n",
        "    - 가중합으로 context vector 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        # Scaled dot-product attention을 위한 스케일 팩터\n",
        "        self.scale = 1.0 / math.sqrt(hidden_dim)\n",
        "\n",
        "    def forward(self, dec_h, enc_out, src_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dec_h: 디코더의 현재 hidden state [batch, 1, hidden_dim]\n",
        "            enc_out: 인코더의 모든 출력 [batch, src_len, hidden_dim]\n",
        "            src_mask: 패딩 마스크 [batch, src_len]\n",
        "\n",
        "        Returns:\n",
        "            ctx: context vector [batch, 1, hidden_dim]\n",
        "            attn: attention weights [batch, 1, src_len]\n",
        "        \"\"\"\n",
        "        # 유사도 계산: [batch, 1, hidden] @ [batch, hidden, src_len] = [batch, 1, src_len]\n",
        "        scores = torch.bmm(dec_h, enc_out.transpose(1,2)) * self.scale\n",
        "\n",
        "        # 패딩 위치는 -inf로 마스킹 (softmax 후 0이 됨)\n",
        "        if src_mask is not None:\n",
        "            scores = scores.masked_fill(src_mask[:,None,:]==0, float('-inf'))\n",
        "\n",
        "        # attention weights: softmax로 확률 분포로 변환\n",
        "        attn = scores.softmax(dim=-1)\n",
        "\n",
        "        # context vector: attention weights와 인코더 출력의 가중합\n",
        "        ctx = torch.bmm(attn, enc_out)\n",
        "        return ctx, attn\n",
        "\n",
        "class AttnDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention 메커니즘을 갖춘 LSTM 디코더\n",
        "\n",
        "    역할:\n",
        "    - 이전 토큰과 context vector를 입력받아 다음 토큰 예측\n",
        "    - Attention을 사용하여 소스 문장의 관련 부분에 집중\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # 타겟 단어 임베딩\n",
        "        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_ID)\n",
        "        # LSTM 입력: 임베딩 + context vector\n",
        "        self.rnn = nn.LSTM(embed_dim + hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True,\n",
        "                           dropout=dropout if num_layers>1 else 0.0)\n",
        "        # 최종 출력: hidden state → vocabulary 크기의 로짓\n",
        "        self.fc  = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.attn = Attention(hidden_dim)\n",
        "\n",
        "    def forward(self, inp_tok, hidden, enc_out, src_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inp_tok: 현재 입력 토큰 [batch]\n",
        "            hidden: 이전 스텝의 (h, c) 상태\n",
        "            enc_out: 인코더 출력 [batch, src_len, hidden_dim]\n",
        "            src_mask: 소스 마스크 [batch, src_len]\n",
        "\n",
        "        Returns:\n",
        "            logits: 다음 토큰 예측 [batch, vocab_size]\n",
        "            hidden: 업데이트된 (h, c) 상태\n",
        "            attn: attention weights [batch, 1, src_len]\n",
        "        \"\"\"\n",
        "        # 토큰 임베딩: [batch] → [batch, 1, embed_dim]\n",
        "        x = self.emb(inp_tok.unsqueeze(1))\n",
        "\n",
        "        # 현재 hidden state 추출 (마지막 레이어)\n",
        "        h_t = hidden[0][-1].unsqueeze(1)\n",
        "\n",
        "        # Attention으로 context vector 계산\n",
        "        ctx, attn = self.attn(h_t, enc_out, src_mask)\n",
        "\n",
        "        # 임베딩과 context를 결합하여 LSTM 입력 생성\n",
        "        rnn_in = torch.cat([x, ctx], dim=-1)\n",
        "\n",
        "        # LSTM 업데이트\n",
        "        out, hidden = self.rnn(rnn_in, hidden)\n",
        "\n",
        "        # 다음 토큰 로짓 예측\n",
        "        logits = self.fc(out.squeeze(1))\n",
        "        return logits, hidden, attn\n",
        "\n",
        "class Seq2SeqAttn(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder-Decoder with Attention 전체 모델\n",
        "\n",
        "    역할:\n",
        "    - 인코더로 소스 문장 인코딩\n",
        "    - 디코더로 타겟 문장 생성 (autoregressive)\n",
        "    - Teacher forcing을 사용한 학습\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        \"\"\"패딩이 아닌 토큰에 대해 True를 반환하는 마스크 생성\"\"\"\n",
        "        return (src != PAD_ID).to(src.device)\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing=0.5, return_attn=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: 소스 문장 [batch, src_len]\n",
        "            trg: 타겟 문장 [batch, trg_len]\n",
        "            teacher_forcing: teacher forcing 비율 (0.0~1.0)\n",
        "            return_attn: attention weights 반환 여부\n",
        "\n",
        "        Returns:\n",
        "            logits: 각 타임스텝의 예측 [batch, trg_len-1, vocab_size]\n",
        "            (optional) attn: attention weights\n",
        "        \"\"\"\n",
        "        B, Ttrg = trg.size()\n",
        "\n",
        "        # 1. 인코더로 소스 문장 인코딩\n",
        "        enc_out, hidden = self.encoder(src)\n",
        "        src_mask = self.make_src_mask(src)\n",
        "\n",
        "        # 2. 디코더 초기 입력 (첫 번째 토큰은 보통 [CLS])\n",
        "        inp = trg[:,0]\n",
        "\n",
        "        logits_list, attn_list = [], []\n",
        "\n",
        "        # 3. 타겟 문장을 한 토큰씩 생성\n",
        "        for t in range(1, Ttrg):\n",
        "            # 현재 토큰 예측\n",
        "            logits, hidden, attn = self.decoder(inp, hidden, enc_out, src_mask)\n",
        "            logits_list.append(logits.unsqueeze(1))\n",
        "            attn_list.append(attn)\n",
        "\n",
        "            # Teacher forcing: 정답 토큰 vs 예측 토큰\n",
        "            use_tf = (random.random() < teacher_forcing)\n",
        "            next_tok = trg[:,t] if use_tf else logits.argmax(-1)\n",
        "            inp = next_tok\n",
        "\n",
        "        # 모든 타임스텝의 예측을 결합\n",
        "        logits = torch.cat(logits_list, dim=1)\n",
        "\n",
        "        if return_attn:\n",
        "            at = torch.cat(attn_list, dim=1).squeeze(2)\n",
        "            return logits, at\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gCSy4IMphqb"
      },
      "outputs": [],
      "source": [
        "# Cross Entropy Loss 정의 (패딩 토큰은 무시)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
        "\n",
        "\n",
        "def shift_trg_for_loss(trg):\n",
        "    \"\"\"\n",
        "    타겟 시퀀스를 한 칸 왼쪽으로 시프트하여 loss 계산용으로 변환\n",
        "\n",
        "    역할:\n",
        "    - 디코더는 t번째 토큰을 입력받아 t+1번째 토큰을 예측\n",
        "    - 따라서 정답 레이블은 1번째부터 끝까지 (첫 번째 토큰은 제외)\n",
        "\n",
        "    Args:\n",
        "        trg: [batch, trg_len] 타겟 시퀀스\n",
        "\n",
        "    Returns:\n",
        "        [batch, trg_len-1] 시프트된 시퀀스\n",
        "    \"\"\"\n",
        "    return trg[:, 1:].contiguous()\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dl, opt, teacher_forcing, clip=1.0, verbose=True):\n",
        "    \"\"\"\n",
        "    LSTM + Attention 모델의 1 에폭 학습 함수\n",
        "\n",
        "    역할:\n",
        "    - 전체 학습 데이터를 한 번 순회하며 모델 파라미터 업데이트\n",
        "    - Teacher forcing을 사용하여 학습 안정화\n",
        "    - Gradient clipping으로 gradient exploding 방지\n",
        "\n",
        "    Args:\n",
        "        model: Seq2SeqAttn 모델\n",
        "        dl: 학습 데이터로더\n",
        "        opt: Optimizer\n",
        "        teacher_forcing: Teacher forcing 비율 (0.0 ~ 1.0)\n",
        "        clip: Gradient clipping 임계값\n",
        "        verbose: 중간 loss 출력 여부\n",
        "\n",
        "    Returns:\n",
        "        평균 loss 값\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total, n = 0.0, 0\n",
        "\n",
        "    for i, (src, trg) in enumerate(dl):\n",
        "        # 데이터를 GPU로 이동\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        # Gradient 초기화\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # Forward pass (teacher forcing 사용)\n",
        "        out = model(src, trg, teacher_forcing=teacher_forcing)\n",
        "\n",
        "        # 타겟 시프트 (예측과 정답을 맞추기 위해)\n",
        "        tgt = shift_trg_for_loss(trg)\n",
        "\n",
        "        # Loss 계산\n",
        "        # out: [batch, trg_len-1, vocab_size] → flatten to [batch*(trg_len-1), vocab_size]\n",
        "        # tgt: [batch, trg_len-1] → flatten to [batch*(trg_len-1)]\n",
        "        loss = criterion(out.reshape(-1, out.size(-1)), tgt.reshape(-1))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping (gradient exploding 방지)\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # 파라미터 업데이트\n",
        "        opt.step()\n",
        "\n",
        "        # Loss 누적\n",
        "        total += loss.item()\n",
        "        n += 1\n",
        "\n",
        "        # 중간 loss 출력 (매 50 배치마다)\n",
        "        if verbose and (i + 1) % 50 == 0:\n",
        "            print(f\"  Batch {i+1}/{len(dl)}: loss={total/n:.4f}\")\n",
        "\n",
        "    return total / n\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, dl):\n",
        "    \"\"\"\n",
        "    LSTM + Attention 모델의 검증 함수\n",
        "\n",
        "    역할:\n",
        "    - Teacher forcing 없이 검증 데이터에서 loss 계산\n",
        "    - Gradient 계산 비활성화로 메모리 절약\n",
        "\n",
        "    Args:\n",
        "        model: Seq2SeqAttn 모델\n",
        "        dl: 검증 데이터로더\n",
        "\n",
        "    Returns:\n",
        "        평균 loss 값\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total, n = 0.0, 0\n",
        "\n",
        "    for src, trg in dl:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        # Teacher forcing 없이 예측 (검증 시에는 실제 추론과 동일하게)\n",
        "        out = model(src, trg, teacher_forcing=0.0)\n",
        "\n",
        "        tgt = shift_trg_for_loss(trg)\n",
        "        loss = criterion(out.reshape(-1, out.size(-1)), tgt.reshape(-1))\n",
        "\n",
        "        total += loss.item()\n",
        "        n += 1\n",
        "\n",
        "    return total / n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIUGRerWphqb"
      },
      "outputs": [],
      "source": [
        "# ==================== LSTM 데이터 전처리 ====================\n",
        "\n",
        "def encode_text_bert(s: str, max_len=128):\n",
        "    \"\"\"\n",
        "    BERT 토크나이저로 텍스트를 토큰 ID로 변환\n",
        "\n",
        "    역할:\n",
        "    - 텍스트를 WordPiece 토큰으로 분할\n",
        "    - 특수 토큰([CLS], [SEP]) 자동 추가\n",
        "    - 최대 길이로 잘라내기(truncation)\n",
        "\n",
        "    Args:\n",
        "        s: 입력 텍스트\n",
        "        max_len: 최대 시퀀스 길이\n",
        "\n",
        "    Returns:\n",
        "        토큰 ID 리스트\n",
        "    \"\"\"\n",
        "    return tokenizer_bert(\n",
        "        s,\n",
        "        add_special_tokens=True,  # [CLS]와 [SEP] 토큰 추가\n",
        "        max_length=max_len + 2,   # 특수 토큰을 위한 공간 확보\n",
        "        truncation=True,\n",
        "        padding=False,  # 배치 단위로 패딩하므로 여기서는 안 함\n",
        "        return_tensors=None,\n",
        "    )[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLHN093Lphqb"
      },
      "outputs": [],
      "source": [
        "def preprocess_function_bert(batch):\n",
        "    \"\"\"Tokenizer-based preprocessing for the from-scratch Transformer.\"\"\"\n",
        "    src_texts = [x[lang_pair[0]] for x in batch[\"translation\"]]\n",
        "    tgt_texts = [x[lang_pair[1]] for x in batch[\"translation\"]]\n",
        "\n",
        "    model_inputs = tokenizer_bert(\n",
        "        src_texts,\n",
        "        max_length=max_length_bert,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    with tokenizer_bert.as_target_tokenizer():\n",
        "        labels = tokenizer_bert(\n",
        "            tgt_texts,\n",
        "            max_length=max_length_bert,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "        )[\"input_ids\"]\n",
        "\n",
        "    labels = [\n",
        "        [(lid if lid != tokenizer_bert.pad_token_id else -100) for lid in label]\n",
        "        for label in labels\n",
        "    ]\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def preprocess_bert_dataset(dataset_dict):\n",
        "    \"\"\"Map the BERT preprocessing across the dataset before training.\"\"\"\n",
        "    ds = dataset_dict if hasattr(dataset_dict, \"map\") else DatasetDict(dataset_dict)\n",
        "    return ds.map(\n",
        "        preprocess_function_bert,\n",
        "        batched=True,\n",
        "        remove_columns=ds[\"train\"].column_names,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYq-KOmgphqb"
      },
      "source": [
        "### LSTM 데이터셋 및 DataLoader 준비\n",
        "\n",
        "LSTM 모델은 커스텀 데이터셋과 collate 함수가 필요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbr_bhJpphqb"
      },
      "outputs": [],
      "source": [
        "class EnDeDataset(Dataset):\n",
        "    \"\"\"\n",
        "    영어-프랑스어 번역 데이터셋 (LSTM용)\n",
        "\n",
        "    역할:\n",
        "    - Hugging Face Dataset을 PyTorch Dataset으로 변환\n",
        "    - 각 샘플을 토크나이즈하여 텐서로 반환\n",
        "    \"\"\"\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            src_ids: 소스 문장의 토큰 ID 텐서\n",
        "            trg_ids: 타겟 문장의 토큰 ID 텐서\n",
        "        \"\"\"\n",
        "        tr = self.pairs[idx][\"translation\"]\n",
        "        src_ids = torch.tensor(encode_text_bert(tr[\"en\"]), dtype=torch.long)\n",
        "        trg_ids = torch.tensor(encode_text_bert(tr[\"fr\"]), dtype=torch.long)\n",
        "        return src_ids, trg_ids\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    \"\"\"\n",
        "    가변 길이 시퀀스를 배치로 묶는 collate 함수\n",
        "\n",
        "    역할:\n",
        "    - 배치 내에서 가장 긴 시퀀스에 맞춰 패딩 추가\n",
        "    - PyTorch의 pad_sequence 사용\n",
        "\n",
        "    Args:\n",
        "        batch: [(src_ids, trg_ids), ...] 리스트\n",
        "\n",
        "    Returns:\n",
        "        srcs_pad: 패딩된 소스 배치 [batch, max_src_len]\n",
        "        trgs_pad: 패딩된 타겟 배치 [batch, max_trg_len]\n",
        "    \"\"\"\n",
        "    srcs, trgs = zip(*batch)\n",
        "    srcs_pad = pad_sequence(srcs, batch_first=True, padding_value=PAD_ID)\n",
        "    trgs_pad = pad_sequence(trgs, batch_first=True, padding_value=PAD_ID)\n",
        "    return srcs_pad, trgs_pad\n",
        "\n",
        "\n",
        "def prepare_lstm_dataloaders(dataset_dict, batch_size=16):\n",
        "    \"\"\"\n",
        "    LSTM 모델용 데이터로더 생성\n",
        "\n",
        "    역할:\n",
        "    - Dataset 객체 생성\n",
        "    - DataLoader로 배치 단위 이터레이터 생성\n",
        "\n",
        "    Args:\n",
        "        dataset_dict: Hugging Face DatasetDict\n",
        "        batch_size: 배치 크기\n",
        "\n",
        "    Returns:\n",
        "        train_dl, valid_dl: 학습 및 검증 데이터로더\n",
        "    \"\"\"\n",
        "    ds = dataset_dict if hasattr(dataset_dict, \"items\") else DatasetDict(dataset_dict)\n",
        "    train_ds = EnDeDataset(ds[\"train\"])\n",
        "    valid_ds = EnDeDataset(ds[\"validation\"])\n",
        "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "    valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "    return train_dl, valid_dl\n",
        "\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# LSTM용 데이터로더 생성\n",
        "train_dl_lstm, valid_dl_lstm = prepare_lstm_dataloaders(dataset_small, batch_size=BATCH_SIZE)\n",
        "print('LSTM용 데이터로더 전처리가 완료되었습니다.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yORAnX6phqb"
      },
      "outputs": [],
      "source": [
        "sample_pair = dataset_small['train'][0]['translation']\n",
        "print('원문 EN:', sample_pair['en'])\n",
        "print('정답 FR:', sample_pair['fr'])\n",
        "\n",
        "encoded_ids = encode_text_bert(sample_pair['en'])\n",
        "print('토큰 ID 예시 (앞 12개):', encoded_ids[:12])\n",
        "print('총 토큰 수:', len(encoded_ids))\n",
        "\n",
        "tmp_train_dl, tmp_valid_dl = prepare_lstm_dataloaders(dataset_small, batch_size=2)\n",
        "batch_src, batch_trg = next(iter(tmp_train_dl))\n",
        "print('배치 텐서 크기:', batch_src.shape, batch_trg.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sykueEhphqb"
      },
      "source": [
        "### LSTM 모델 학습 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO3SUNE6phqb"
      },
      "outputs": [],
      "source": [
        "# ==================== LSTM 모델 초기화 및 학습 ====================\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "EMBED_DIM = 256      # 임베딩 차원\n",
        "HIDDEN_DIM = 512     # LSTM hidden state 차원\n",
        "NUM_LAYERS = 1       # LSTM 레이어 개수\n",
        "DROPOUT = 0.1        # Dropout 비율\n",
        "LR = 5e-3            # Learning rate\n",
        "EPOCHS = 3           # 학습 에폭 수\n",
        "CLIP = 1.0           # Gradient clipping 임계값\n",
        "TEACHER_FORCING = 1.0  # Teacher forcing 비율 (1.0 = 100% 정답 사용)\n",
        "\n",
        "# 체크포인트 저장 디렉토리\n",
        "SAVE_DIR = \"./checkpoints_enfr\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# 모델 생성\n",
        "enc = Encoder(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT)\n",
        "dec = AttnDecoder(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT)\n",
        "model_attn = Seq2SeqAttn(enc, dec).to(device)\n",
        "\n",
        "# Optimizer: AdamW (Adam + Weight Decay)\n",
        "opt_attn = torch.optim.AdamW(model_attn.parameters(), lr=LR)\n",
        "\n",
        "print(f\"[Model A: LSTM+Attention] Total params: {sum(p.numel() for p in model_attn.parameters()):,}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 학습 루프\n",
        "best_val = 1e9\n",
        "attn_ckpt = os.path.join(SAVE_DIR, \"seq2seq_attn.pt\")\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    print(f\"\\n[LSTM+Attn] Epoch {ep}/{EPOCHS}\")\n",
        "\n",
        "    # 학습\n",
        "    tr = train_one_epoch(model_attn, train_dl_lstm, opt_attn, TEACHER_FORCING, CLIP, verbose=True)\n",
        "\n",
        "    # 검증\n",
        "    va = eval_model(model_attn, valid_dl_lstm)\n",
        "\n",
        "    # Best 모델 저장\n",
        "    if va < best_val:\n",
        "        best_val = va\n",
        "        torch.save(model_attn.state_dict(), attn_ckpt)\n",
        "        print(f\"  Best model saved! train={tr:.4f}  valid={va:.4f}\")\n",
        "    else:\n",
        "        print(f\"  train={tr:.4f}  valid={va:.4f}\")\n",
        "\n",
        "print(f\"\\n{'=' * 80}\")\n",
        "print(f\"LSTM+Attention 학습 완료! Best valid loss: {best_val:.4f}\")\n",
        "print(f\"Saved: {attn_ckpt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-dts9Zwphqb"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def translate_attn(model, text: str, max_new_tokens=60, collect_attn=True):\n",
        "    \"\"\"\n",
        "    LSTM+Attention 모델로 번역 수행\n",
        "\n",
        "    역할:\n",
        "    - 영어 문장을 프랑스어로 번역\n",
        "    - Autoregressive 방식으로 한 토큰씩 생성\n",
        "    - Attention weights 수집하여 시각화 가능\n",
        "\n",
        "    Args:\n",
        "        model: Seq2SeqAttn 모델\n",
        "        text: 입력 영어 문장\n",
        "        max_new_tokens: 생성할 최대 토큰 수\n",
        "        collect_attn: Attention weights 수집 여부\n",
        "\n",
        "    Returns:\n",
        "        text_out: 생성된 프랑스어 번역\n",
        "        attn_mat: Attention 가중치 행렬 [trg_len, src_len]\n",
        "        src: 소스 토큰 ID 배열\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 소스 문장 토크나이즈\n",
        "    src = torch.tensor(encode_text_bert(text), dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # 인코더로 소스 인코딩\n",
        "    enc_out, hidden = model.encoder(src)\n",
        "    src_mask = (src != PAD_ID)\n",
        "\n",
        "    # 디코더 초기 입력 ([CLS] 토큰)\n",
        "    inp = torch.tensor([CLS_ID], device=device)\n",
        "\n",
        "    pred_ids, steps = [], []\n",
        "\n",
        "    # 토큰 하나씩 생성\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits, hidden, attn = model.decoder(inp, hidden, enc_out, src_mask)\n",
        "\n",
        "        # 가장 높은 확률의 토큰 선택 (greedy decoding)\n",
        "        nxt = int(logits.argmax(-1).item())\n",
        "\n",
        "        # [SEP] 토큰이 나오면 종료\n",
        "        if nxt == SEP_ID:\n",
        "            break\n",
        "\n",
        "        pred_ids.append(nxt)\n",
        "\n",
        "        # Attention weights 수집\n",
        "        if collect_attn:\n",
        "            steps.append(attn.squeeze(0).squeeze(0).detach().cpu().numpy())\n",
        "\n",
        "        # 다음 스텝의 입력으로 사용\n",
        "        inp = torch.tensor([nxt], device=device)\n",
        "\n",
        "    # 토큰 ID를 텍스트로 디코딩\n",
        "    text_out = tokenizer_bert.decode(pred_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Attention 행렬 생성\n",
        "    attn_mat = np.stack(steps, axis=0) if (collect_attn and len(steps) > 0) else None\n",
        "\n",
        "    return text_out, attn_mat, src.squeeze(0).cpu().numpy()\n",
        "\n",
        "\n",
        "def tokens_from_ids(ids, tokenizer):\n",
        "    \"\"\"\n",
        "    토큰 ID를 토큰 문자열로 변환 (특수 토큰 제외)\n",
        "\n",
        "    역할:\n",
        "    - 시각화를 위해 토큰 ID를 읽기 쉬운 문자열로 변환\n",
        "    - [CLS], [SEP], [PAD] 등 특수 토큰은 제외\n",
        "\n",
        "    Args:\n",
        "        ids: 토큰 ID 배열\n",
        "        tokenizer: 토크나이저 객체\n",
        "\n",
        "    Returns:\n",
        "        필터링된 토큰 리스트\n",
        "    \"\"\"\n",
        "    toks = tokenizer.convert_ids_to_tokens(ids)\n",
        "    return [t for t in toks if t not in (tokenizer.cls_token, tokenizer.sep_token,\n",
        "                                         tokenizer.pad_token, tokenizer.unk_token)]\n",
        "\n",
        "\n",
        "def plot_attention_lstm(src_tokens, trg_tokens, attn, title=\"LSTM Attention Heatmap\"):\n",
        "    \"\"\"\n",
        "    LSTM Attention 가중치를 히트맵으로 시각화\n",
        "\n",
        "    역할:\n",
        "    - 각 타겟 토큰이 어떤 소스 토큰에 집중하는지 시각화\n",
        "    - x축: 소스 문장 토큰\n",
        "    - y축: 타겟 문장 토큰\n",
        "    - 색상: Attention 가중치 (밝을수록 높은 가중치)\n",
        "\n",
        "    Args:\n",
        "        src_tokens: 소스 토큰 리스트\n",
        "        trg_tokens: 타겟 토큰 리스트\n",
        "        attn: Attention 가중치 행렬 [trg_len, src_len]\n",
        "        title: 그래프 제목\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(min(12, 0.5 * len(src_tokens) + 3), min(10, 0.5 * len(trg_tokens) + 3)))\n",
        "    plt.imshow(attn[:len(trg_tokens), :len(src_tokens)], aspect='auto', cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.yticks(range(len(trg_tokens)), trg_tokens, fontsize=10)\n",
        "    plt.xticks(range(len(src_tokens)), src_tokens, rotation=45, ha='right', fontsize=9)\n",
        "    plt.xlabel('Source (EN) [WordPiece]')\n",
        "    plt.ylabel('Target (DE) [WordPiece]')\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0xkZMV0phqc"
      },
      "source": [
        "### LSTM+Attention 예시 번역 및 Attention Map 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOrFqCCBphqc"
      },
      "outputs": [],
      "source": [
        "# 테스트 문장\n",
        "test_sent = \"We will travel to Berlin next summer.\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"[Model A: LSTM+Attention] 예시 번역\")\n",
        "print(\"=\"*80)\n",
        "print(f\"EN: {test_sent}\")\n",
        "\n",
        "de_out, attn_weights, src_ids = translate_attn(model_attn, test_sent, collect_attn=True)\n",
        "print(f\"DE: {de_out}\")\n",
        "\n",
        "if attn_weights is not None:\n",
        "    src_toks = tokens_from_ids(src_ids, tokenizer_bert)\n",
        "    trg_toks = tokenizer_bert.tokenize(de_out)\n",
        "    plot_attention_lstm(src_toks, trg_toks, attn_weights, title=\"Model A: LSTM+Attention Heatmap\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaSBAA8-phqc"
      },
      "source": [
        "---\n",
        "## 6) Model B: From-Scratch Transformer (Encoder-Decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5394a0c3"
      },
      "source": [
        "### Self-Attention과 Transformer 핵심 아이디어\n",
        "Transformer의 셀프 어텐션은 문장 내부 모든 위치 쌍을 동시에 살피면서 관계를 학습합니다.\n",
        "- Query, Key, Value 행렬을 통해 토큰 간 연관도를 계산하고, 멀티헤드 구조로 다양한 표현을 병렬로 추출합니다.\n",
        "- 위치 정보를 더하기 위해 포지셔널 인코딩을 사용하며, RNN 대비 훨씬 깊고 길게 학습하기 수월합니다.\n",
        "- 인코더-디코더 구조에서는 인코더의 self-attention과 디코더의 self-attention, 그리고 cross-attention이 협력하여 번역을 완성합니다.\n",
        "\n",
        "### Transformers\n",
        "\n",
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-uZ5XnEphqc"
      },
      "outputs": [],
      "source": [
        "# ==================== From-Scratch Transformer 모델 정의 ====================\n",
        "\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"위치 인코딩: Transformer에 시퀀스 순서 정보 제공\"\"\"\n",
        "    def __init__(self, d_model, max_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # 위치 인코딩 생성\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"x: (batch_size, seq_len, d_model)\"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class DecoderLayerWithXAttn(nn.TransformerDecoderLayer):\n",
        "    \"\"\"\n",
        "    nn.TransformerDecoderLayer를 확장해서 cross-attention 가중치를 선택적으로 반환\n",
        "    반환 형태: attn (batch, num_heads, tgt_len, src_len)\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(\n",
        "        self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
        "        tgt_key_padding_mask=None, memory_key_padding_mask=None,\n",
        "        need_weights: bool = False,\n",
        "        average_attn_weights: bool = False,\n",
        "    ):\n",
        "        # 1) self-attention (가중치는 필요없음)\n",
        "        x = tgt\n",
        "        x2, _ = self.self_attn(\n",
        "            x, x, x,\n",
        "            attn_mask=tgt_mask,\n",
        "            key_padding_mask=tgt_key_padding_mask,\n",
        "            need_weights=False\n",
        "        )\n",
        "        x = x + self.dropout1(x2)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # 2) cross-attention (여기서 가중치 받기)\n",
        "        x2, xattn = self.multihead_attn(\n",
        "            x, memory, memory,\n",
        "            attn_mask=memory_mask,\n",
        "            key_padding_mask=memory_key_padding_mask,\n",
        "            need_weights=need_weights,\n",
        "            average_attn_weights=average_attn_weights  # False면 헤드별 가중치\n",
        "        )\n",
        "        x = x + self.dropout2(x2)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        # 3) FFN\n",
        "        x2 = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
        "        x = x + self.dropout3(x2)\n",
        "        x = self.norm3(x)\n",
        "\n",
        "        # 반환\n",
        "        if need_weights:\n",
        "            if xattn is not None and xattn.dim() == 2:\n",
        "                xattn = xattn.unsqueeze(0).unsqueeze(0)  # (1,1,T,S)\n",
        "            return x, xattn\n",
        "        return x, None\n",
        "\n",
        "\n",
        "class TransformerDecoderWithXAttn(nn.Module):\n",
        "    \"\"\"\n",
        "    디코더 스택을 돌리며 레이어별 cross-attention 가중치를 수집\n",
        "    \"\"\"\n",
        "    def __init__(self, decoder_layer: nn.Module, num_layers: int):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer) for _ in range(num_layers)])\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(\n",
        "        self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
        "        tgt_key_padding_mask=None, memory_key_padding_mask=None,\n",
        "        need_weights: bool = False,\n",
        "        average_attn_weights: bool = False,\n",
        "    ):\n",
        "        x = tgt\n",
        "        xattn_all = []  # 각 레이어의 (B, H, T, S)\n",
        "        for layer in self.layers:\n",
        "            x, xattn = layer(\n",
        "                x, memory,\n",
        "                tgt_mask=tgt_mask,\n",
        "                memory_mask=memory_mask,\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask,\n",
        "                need_weights=need_weights,\n",
        "                average_attn_weights=average_attn_weights\n",
        "            )\n",
        "            if need_weights:\n",
        "                xattn_all.append(xattn)\n",
        "        return x, xattn_all if need_weights else None\n",
        "\n",
        "\n",
        "class TransformerSeq2Seq(nn.Module):\n",
        "    \"\"\"PyTorch Transformer 기반 Seq2Seq 모델 (cross-attn 가중치 수집 지원)\"\"\"\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=4, num_encoder_layers=4,\n",
        "                 num_decoder_layers=4, dim_feedforward=512, dropout=0.1, max_len=512):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.nhead = nhead\n",
        "\n",
        "        # 임베딩 레이어\n",
        "        self.src_embedding = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n",
        "        self.tgt_embedding = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n",
        "\n",
        "        # 위치 인코딩\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len, dropout)\n",
        "\n",
        "        # Transformer 인코더\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
        "\n",
        "        # Transformer 디코더 (확장 버전)\n",
        "        decoder_layer = DecoderLayerWithXAttn(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_decoder = TransformerDecoderWithXAttn(decoder_layer, num_decoder_layers)\n",
        "\n",
        "        # 출력 레이어\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        return (src == PAD_ID)  # (batch, src_len)\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        batch_size, tgt_len = tgt.size()\n",
        "        tgt_mask = torch.triu(torch.ones(tgt_len, tgt_len, device=tgt.device), diagonal=1).bool()\n",
        "        tgt_padding_mask = (tgt == PAD_ID)\n",
        "        return tgt_mask, tgt_padding_mask\n",
        "\n",
        "    def forward(self, src, tgt, *, need_xattn: bool = False, per_head: bool = True):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            logits: (batch, tgt_len, vocab_size)\n",
        "            xattn_all: Optional[List[(batch, num_heads, tgt_len, src_len)]]\n",
        "        \"\"\"\n",
        "        src_padding_mask = self.make_src_mask(src)\n",
        "        tgt_mask, tgt_padding_mask = self.make_tgt_mask(tgt)\n",
        "\n",
        "        # 임베딩 + 위치 인코딩\n",
        "        src_emb = self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))\n",
        "        tgt_emb = self.pos_encoder(self.tgt_embedding(tgt) * math.sqrt(self.d_model))\n",
        "\n",
        "        # 인코더\n",
        "        memory = self.transformer_encoder(src_emb, src_key_padding_mask=src_padding_mask)\n",
        "\n",
        "        # 디코더 (가중치 수집 on/off)\n",
        "        dec_out, xattn_all = self.transformer_decoder(\n",
        "            tgt_emb,\n",
        "            memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=src_padding_mask,\n",
        "            need_weights=need_xattn,\n",
        "            average_attn_weights=(not per_head)  # False면 (B,H,T,S)\n",
        "        )\n",
        "\n",
        "        # 출력\n",
        "        logits = self.fc_out(dec_out)\n",
        "        return (logits, xattn_all) if need_xattn else (logits, None)\n",
        "\n",
        "\n",
        "# 모델 초기화 예시 (나중에 실제 학습 시 사용)\n",
        "model_scratch = TransformerSeq2Seq(\n",
        "    vocab_size=len(tokenizer_bert),\n",
        "    d_model=256,\n",
        "    nhead=4,\n",
        "    num_encoder_layers=3,\n",
        "    num_decoder_layers=3,\n",
        "    dim_feedforward=512,\n",
        "    dropout=0.1\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU5ywXYwphqc"
      },
      "source": [
        "### From-Scratch Transformer 데이터 전처리 및 학습\n",
        "\n",
        "Transformer는 Hugging Face Trainer를 사용하므로 데이터셋 전처리가 필요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_07IYy9phqc"
      },
      "outputs": [],
      "source": [
        "# ==================== From-Scratch Transformer 학습 ====================\n",
        "\n",
        "# Transformer용 학습 함수\n",
        "def train_transformer_epoch(model, dataloader, optimizer, clip=1.0):\n",
        "    \"\"\"Transformer 모델 1 epoch 학습\"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        src = batch[0].to(device)  # (batch_size, src_len)\n",
        "        tgt = batch[1].to(device)  # (batch_size, tgt_len)\n",
        "\n",
        "        # Teacher forcing: 디코더 입력은 타겟의 [:-1] (마지막 토큰 제외)\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]  # 예측해야 할 타겟 (첫 토큰 제외)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        output, _ = model(src, tgt_input, need_xattn=False)  # (batch_size, tgt_len-1, vocab_size)\n",
        "\n",
        "        # Loss 계산\n",
        "        output = output.reshape(-1, output.shape[-1])  # (batch_size * tgt_len, vocab_size)\n",
        "        tgt_output = tgt_output.reshape(-1)  # (batch_size * tgt_len)\n",
        "\n",
        "        loss = nn.CrossEntropyLoss(ignore_index=PAD_ID)(output, tgt_output)\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "def eval_transformer(model, dataloader):\n",
        "    \"\"\"Transformer 모델 검증\"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            src = batch[0].to(device)\n",
        "            tgt = batch[1].to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            output, _ = model(src, tgt_input, need_xattn=False)\n",
        "\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "            loss = nn.CrossEntropyLoss(ignore_index=PAD_ID)(output, tgt_output)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "# 데이터로더 준비 (LSTM과 동일한 데이터 사용 가능)\n",
        "train_dl_transformer, valid_dl_transformer = prepare_lstm_dataloaders(dataset_small, batch_size=32)\n",
        "\n",
        "# 옵티마이저 설정\n",
        "optimizer_scratch = torch.optim.Adam(model_scratch.parameters(), lr=3e-4)\n",
        "\n",
        "# 학습 설정\n",
        "EPOCHS_TRANSFORMER = 5\n",
        "CLIP = 1.0\n",
        "\n",
        "# 체크포인트 디렉토리\n",
        "scratch_ckpt_dir = \"./checkpoints_enfr/from_scratch_transformer\"\n",
        "os.makedirs(scratch_ckpt_dir, exist_ok=True)\n",
        "scratch_ckpt_path = os.path.join(scratch_ckpt_dir, \"model.pt\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"From-Scratch Transformer 학습 시작\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(1, EPOCHS_TRANSFORMER + 1):\n",
        "    print(f\"\\n[Epoch {epoch}/{EPOCHS_TRANSFORMER}]\")\n",
        "\n",
        "    # 학습\n",
        "    train_loss = train_transformer_epoch(model_scratch, train_dl_transformer, optimizer_scratch, CLIP)\n",
        "\n",
        "    # 검증\n",
        "    valid_loss = eval_transformer(model_scratch, valid_dl_transformer)\n",
        "\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Valid Loss: {valid_loss:.4f}\")\n",
        "\n",
        "    # Best 모델 저장\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model_scratch.state_dict(), scratch_ckpt_path)\n",
        "        print(f\"  ✓ Best 모델 저장: {scratch_ckpt_path}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"From-Scratch Transformer 학습 완료!\")\n",
        "print(f\"Best Validation Loss: {best_valid_loss:.4f}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5WpOGjZphqc"
      },
      "source": [
        "### From-Scratch Transformer 번역 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6-AO822phqc"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def translate_transformer_scratch(model, text: str, max_new_tokens=64, return_attention=False):\n",
        "    \"\"\"\n",
        "    From-Scratch Transformer로 번역 수행 (Greedy Decoding)\n",
        "\n",
        "    Args:\n",
        "        model: TransformerSeq2Seq 모델\n",
        "        text: 입력 영어 문장\n",
        "        max_new_tokens: 생성할 최대 토큰 수\n",
        "        return_attention: Attention weights 반환 여부\n",
        "\n",
        "    Returns:\n",
        "        translation: 번역된 텍스트\n",
        "        (선택) attention_weights: Cross-attention weights (if return_attention=True)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 입력 텍스트 토크나이즈\n",
        "    src_ids = encode_text_bert(text)\n",
        "    src = torch.LongTensor([src_ids]).to(device)  # (1, src_len)\n",
        "\n",
        "    # 디코더 시작: [CLS] 토큰\n",
        "    tgt_ids = [CLS_ID]\n",
        "\n",
        "    # Autoregressive 생성\n",
        "    for _ in range(max_new_tokens):\n",
        "        tgt = torch.LongTensor([tgt_ids]).to(device)  # (1, tgt_len)\n",
        "\n",
        "        # 예측 (attention은 마지막 forward에서만 가져오면 됨)\n",
        "        output, _ = model(src, tgt, need_xattn=False)  # (1, tgt_len, vocab_size)\n",
        "\n",
        "        # 마지막 토큰의 예측 확률\n",
        "        next_token_logits = output[0, -1, :]  # (vocab_size,)\n",
        "        next_token_id = next_token_logits.argmax().item()\n",
        "\n",
        "        # 종료 조건: [SEP] 토큰 생성\n",
        "        if next_token_id == SEP_ID:\n",
        "            break\n",
        "\n",
        "        tgt_ids.append(next_token_id)\n",
        "\n",
        "    # 디코딩\n",
        "    translation = tokenizer_bert.decode(tgt_ids, skip_special_tokens=True)\n",
        "\n",
        "    if return_attention:\n",
        "        # 최종 생성된 문장으로 한 번 더 forward pass해서 attention 추출\n",
        "        tgt_final = torch.LongTensor([tgt_ids]).to(device)\n",
        "        _, xattn_all = model(src, tgt_final, need_xattn=True, per_head=True)\n",
        "\n",
        "        # xattn_all: List[(batch, num_heads, tgt_len, src_len)]\n",
        "        # 마지막 레이어의 attention 반환\n",
        "        return translation, xattn_all, src_ids, tgt_ids\n",
        "\n",
        "    return translation\n",
        "\n",
        "\n",
        "# 간단한 테스트\n",
        "test_sentence = \"Hello, how are you?\"\n",
        "print(f\"\\n테스트 번역:\")\n",
        "print(f\"  EN: {test_sentence}\")\n",
        "try:\n",
        "    translation = translate_transformer_scratch(model_scratch, test_sentence)\n",
        "    print(f\"  FR: {translation}\")\n",
        "except Exception as e:\n",
        "    print(f\"  오류: {e}\")\n",
        "    print(\"  (학습 후 정상 작동합니다)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQFmYEzcphqc"
      },
      "outputs": [],
      "source": [
        "def plot_cross_attention_transformer(model, text: str, layer=0, head=0):\n",
        "    \"\"\"\n",
        "    From-Scratch Transformer의 Cross-attention 시각화\n",
        "\n",
        "    역할:\n",
        "    - Transformer 디코더의 cross-attention 가중치를 히트맵으로 표시\n",
        "    - BERT WordPiece 토큰 단위로 시각화\n",
        "\n",
        "    Args:\n",
        "        model: TransformerSeq2Seq 모델\n",
        "        text: 입력 텍스트\n",
        "        layer: 시각화할 디코더 레이어 인덱스 (0부터 시작)\n",
        "        head: 시각화할 attention head 인덱스 (0부터 시작)\n",
        "    \"\"\"\n",
        "    # Attention과 함께 번역 생성\n",
        "    translation, xattn_all, src_ids, tgt_ids = translate_transformer_scratch(\n",
        "        model, text, return_attention=True\n",
        "    )\n",
        "\n",
        "    if xattn_all is None or len(xattn_all) == 0:\n",
        "        print(\"⚠️  Attention weights를 가져올 수 없습니다.\")\n",
        "        return\n",
        "\n",
        "    # xattn_all: List[(batch, num_heads, tgt_len, src_len)]\n",
        "    if layer >= len(xattn_all):\n",
        "        print(f\"⚠️  Layer {layer}는 존재하지 않습니다. 총 {len(xattn_all)}개 레이어.\")\n",
        "        return\n",
        "\n",
        "    # 선택한 레이어의 attention: (batch, num_heads, tgt_len, src_len)\n",
        "    layer_attn = xattn_all[layer]\n",
        "\n",
        "    if layer_attn is None:\n",
        "        print(f\"⚠️  Layer {layer}의 attention이 None입니다.\")\n",
        "        return\n",
        "\n",
        "    # CPU로 이동하고 numpy 변환\n",
        "    attn = layer_attn[0].detach().cpu().numpy()  # (num_heads, tgt_len, src_len)\n",
        "\n",
        "    if head >= attn.shape[0]:\n",
        "        print(f\"⚠️  Head {head}는 존재하지 않습니다. 총 {attn.shape[0]}개 헤드.\")\n",
        "        return\n",
        "\n",
        "    # 선택한 헤드의 attention: (tgt_len, src_len)\n",
        "    attn_head = attn[head]\n",
        "\n",
        "    # 토큰 변환\n",
        "    src_tokens = tokenizer_bert.convert_ids_to_tokens(src_ids)\n",
        "    tgt_tokens = tokenizer_bert.convert_ids_to_tokens(tgt_ids)\n",
        "\n",
        "    # 히트맵 시각화\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(min(14, 0.6 * len(src_tokens) + 3), min(12, 0.6 * len(tgt_tokens) + 3)))\n",
        "    plt.imshow(attn_head, aspect='auto', cmap='viridis', interpolation='nearest')\n",
        "    plt.colorbar(label='Attention Weight')\n",
        "\n",
        "    # 축 레이블\n",
        "    plt.yticks(range(len(tgt_tokens)), tgt_tokens, fontsize=10)\n",
        "    plt.xticks(range(len(src_tokens)), src_tokens, rotation=45, ha='right', fontsize=9)\n",
        "\n",
        "    plt.xlabel('Source (EN) [BERT WordPiece]', fontsize=11)\n",
        "    plt.ylabel('Target (FR) [BERT WordPiece]', fontsize=11)\n",
        "    plt.title(f\"Model B: From-Scratch Transformer Cross-Attention\\nLayer {layer}, Head {head}\", fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n번역 결과:\")\n",
        "    print(f\"  EN: {text}\")\n",
        "    print(f\"  FR: {translation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzaSnQyDphqc"
      },
      "source": [
        "### From-Scratch Transformer 예시 번역 및 Attention Map 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eHZcClbphqc"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"[Model B: From-Scratch Transformer] 예시 번역\")\n",
        "print(\"=\"*80)\n",
        "print(f\"EN: {test_sent}\")\n",
        "\n",
        "de_out_scratch = translate_transformer_scratch(model_scratch, test_sent)\n",
        "print(f\"DE: {de_out_scratch[0]}\")\n",
        "\n",
        "# Cross-attention 시각화 (주석 해제하여 사용)\n",
        "plot_cross_attention_transformer(model_scratch, test_sent, layer=0, head=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IoC74MFphqc"
      },
      "source": [
        "---\n",
        "## 7) Model C: Pretrained `t5-small` 미세조정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1efa2eca"
      },
      "source": [
        "### T5 모델 개요\n",
        "T5(Text-to-Text Transfer Transformer)는 모든 NLP 문제를 텍스트 입력과 텍스트 출력의 형태로 통일한 모델입니다.\n",
        "- 거대한 사전학습(Colossal Clean Crawled Corpus)으로 다수의 언어 작업을 학습했으며, 프롬프트(prefix)를 달리해 태스크를 구분합니다.\n",
        "- 미세조정 시에는 프리픽스를 붙여 문제를 설명하고, 동일한 모델 구조로 번역/요약/문제해결 등 다양한 작업을 수행할 수 있습니다.\n",
        "- 본 노트북에서는 `t5-small` 체크포인트를 불러와 EN→FR 번역 태스크에 맞춰 추가 학습합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPTyTuE9phqm"
      },
      "outputs": [],
      "source": [
        "# ==================== T5 데이터 전처리 ====================\n",
        "\n",
        "def preprocess_function_t5(batch):\n",
        "    \"\"\"\n",
        "    T5 모델용 데이터 전처리 함수\n",
        "\n",
        "    역할:\n",
        "    - Task prefix 추가 (\"translate English to French: \")\n",
        "    - 인코더와 디코더 입력 생성\n",
        "    - Labels에서 패딩 토큰은 -100으로 대체\n",
        "\n",
        "    Args:\n",
        "        batch: 배치 딕셔너리\n",
        "\n",
        "    Returns:\n",
        "        전처리된 딕셔너리 (input_ids, attention_mask, labels)\n",
        "    \"\"\"\n",
        "    # Task prefix를 소스 텍스트 앞에 추가\n",
        "    src_texts = [task_prefix + x[lang_pair[0]] for x in batch[\"translation\"]]\n",
        "    tgt_texts = [x[lang_pair[1]] for x in batch[\"translation\"]]\n",
        "\n",
        "    # 인코더 입력 토크나이즈\n",
        "    model_inputs = tokenizer_t5(\n",
        "        src_texts,\n",
        "        max_length=max_source_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    # 디코더 입력(타겟) 토크나이즈\n",
        "    with tokenizer_t5.as_target_tokenizer():\n",
        "        labels = tokenizer_t5(\n",
        "            tgt_texts,\n",
        "            max_length=max_target_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "    # 패딩 토큰을 -100으로 변경 (loss 계산 시 무시)\n",
        "    labels[\"input_ids\"] = [\n",
        "        [(lid if lid != tokenizer_t5.pad_token_id else -100) for lid in label]\n",
        "        for label in labels[\"input_ids\"]\n",
        "    ]\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def preprocess_t5_dataset(dataset_dict):\n",
        "    \"\"\"\n",
        "    T5용 데이터셋 전처리\n",
        "\n",
        "    역할:\n",
        "    - 전체 데이터셋에 T5 전처리 적용\n",
        "\n",
        "    Args:\n",
        "        dataset_dict: Hugging Face DatasetDict\n",
        "\n",
        "    Returns:\n",
        "        전처리된 DatasetDict\n",
        "    \"\"\"\n",
        "    ds = dataset_dict if hasattr(dataset_dict, \"map\") else DatasetDict(dataset_dict)\n",
        "    return ds.map(\n",
        "        preprocess_function_t5,\n",
        "        batched=True,\n",
        "        remove_columns=ds[\"train\"].column_names,\n",
        "    )\n",
        "\n",
        "# T5용 데이터셋 전처리\n",
        "tokenized_t5 = preprocess_t5_dataset(dataset_small)\n",
        "print('T5 전처리가 완료되었습니다.')\n",
        "\n",
        "# ==================== T5 모델 학습 ====================\n",
        "\n",
        "# 사전학습된 T5-small 모델 로드\n",
        "model_ft = AutoModelForSeq2SeqLM.from_pretrained(model_name_t5).to(device)\n",
        "\n",
        "# Data collator\n",
        "data_collator_t5 = DataCollatorForSeq2Seq(tokenizer=tokenizer_t5, model=model_ft)\n",
        "\n",
        "# 학습 인자 설정\n",
        "train_args_ft = TrainingArguments(\n",
        "    output_dir=\"./out_t5_ft\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=3e-4,              # T5는 낮은 learning rate 사용\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=3,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=100,\n",
        "    predict_with_generate=True,      # 평가 시 생성 모드\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Trainer 초기화\n",
        "trainer_ft = Trainer(\n",
        "    model=model_ft,\n",
        "    args=train_args_ft,\n",
        "    train_dataset=tokenized_t5[\"train\"],\n",
        "    eval_dataset=tokenized_t5[\"validation\"],\n",
        "    tokenizer=tokenizer_t5,\n",
        "    data_collator=data_collator_t5,\n",
        "    compute_metrics=compute_metrics_t5,\n",
        ")\n",
        "\n",
        "print(f\"[Model C: Fine-tuned T5] Total params: {sum(p.numel() for p in model_ft.parameters()):,}\")\n",
        "print('학습을 시작합니다...')\n",
        "trainer_ft.train()\n",
        "\n",
        "# 학습 후 모델 저장\n",
        "model_ft.save_pretrained(\"./checkpoints_enfr/t5_finetuned\")\n",
        "tokenizer_t5.save_pretrained(\"./checkpoints_enfr/t5_finetuned\")\n",
        "print('T5 Fine-tuning 완료!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srEtg1H6phqm"
      },
      "source": [
        "### T5 번역 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fZSy3H3phqm"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def translate_t5(model, texts, max_new_tokens=64, num_beams=4, output_attentions=False):\n",
        "    \"\"\"\n",
        "    Fine-tuned T5 모델로 번역 수행\n",
        "\n",
        "    역할:\n",
        "    - Task prefix를 포함하여 T5 형식에 맞게 입력 구성\n",
        "    - Beam search로 다양한 번역 후보 탐색 후 최선 선택\n",
        "\n",
        "    Args:\n",
        "        model: T5ForConditionalGeneration 모델\n",
        "        texts: 입력 텍스트 리스트\n",
        "        max_new_tokens: 생성할 최대 토큰 수\n",
        "        num_beams: Beam search beam 개수\n",
        "        output_attentions: Attention weights 반환 여부\n",
        "\n",
        "    Returns:\n",
        "        번역 텍스트 리스트 (또는 텍스트와 생성 출력 튜플)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Task prefix 추가\n",
        "    inputs = tokenizer_t5([task_prefix + t for t in texts], return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    # 생성 인자 설정\n",
        "    gen_kwargs = {\"max_new_tokens\": max_new_tokens, \"num_beams\": num_beams}\n",
        "    if output_attentions:\n",
        "        gen_kwargs[\"output_attentions\"] = True\n",
        "        gen_kwargs[\"return_dict_in_generate\"] = True\n",
        "\n",
        "    # 번역 생성\n",
        "    gen = model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "    if output_attentions:\n",
        "        return tokenizer_t5.batch_decode(gen.sequences, skip_special_tokens=True), gen\n",
        "    return tokenizer_t5.batch_decode(gen, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def plot_cross_attention_t5(model, text: str, layer=0, head=0):\n",
        "    \"\"\"\n",
        "    T5 모델의 Cross-attention 시각화\n",
        "\n",
        "    역할:\n",
        "    - T5 디코더의 cross-attention 가중치를 히트맵으로 표시\n",
        "    - SentencePiece 토큰 단위로 시각화\n",
        "\n",
        "    Args:\n",
        "        model: T5ForConditionalGeneration 모델\n",
        "        text: 입력 텍스트\n",
        "        layer: 시각화할 디코더 레이어 인덱스\n",
        "        head: 시각화할 attention head 인덱스\n",
        "    \"\"\"\n",
        "    # Attention과 함께 번역 생성\n",
        "    translations, gen_out = translate_t5(model, [text], output_attentions=True)\n",
        "\n",
        "    if hasattr(gen_out, 'cross_attentions') and gen_out.cross_attentions:\n",
        "        # Cross-attention 추출\n",
        "        cross_attn = gen_out.cross_attentions[-1][layer][0].detach().cpu().numpy()\n",
        "        attn_head = cross_attn[head]\n",
        "\n",
        "        # T5 토크나이저로 토큰 추출\n",
        "        src_tokens = tokenizer_t5.tokenize(task_prefix + text)\n",
        "        tgt_tokens = tokenizer_t5.tokenize(translations[0])\n",
        "\n",
        "        # 히트맵 시각화\n",
        "        plt.figure(figsize=(min(12, 0.5 * len(src_tokens) + 3), min(10, 0.5 * len(tgt_tokens) + 3)))\n",
        "        plt.imshow(attn_head[:len(tgt_tokens), :len(src_tokens)], aspect='auto', cmap='viridis')\n",
        "        plt.colorbar()\n",
        "        plt.yticks(range(len(tgt_tokens)), tgt_tokens, fontsize=10)\n",
        "        plt.xticks(range(len(src_tokens)), src_tokens, rotation=45, ha='right', fontsize=9)\n",
        "        plt.xlabel('Source (EN) [SentencePiece]')\n",
        "        plt.ylabel('Target (DE) [SentencePiece]')\n",
        "        plt.title(f\"Model C: T5 Cross-Attention (Layer {layer}, Head {head})\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print('Cross-attention이 반환되지 않았습니다.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf1Xjp94phqm"
      },
      "source": [
        "### Fine-tuned T5 예시 번역 및 Attention Map 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McslpHxyphqn"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"[Model C: Fine-tuned T5] 예시 번역\")\n",
        "print(\"=\"*80)\n",
        "print(f\"EN: {test_sent}\")\n",
        "\n",
        "de_out_t5 = translate_t5(model_ft, [test_sent])\n",
        "print(f\"DE: {de_out_t5[0]}\")\n",
        "\n",
        "# Cross-attention 시각화 (주석 해제하여 사용)\n",
        "plot_cross_attention_t5(model_ft, test_sent, layer=0, head=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF0umygrphqn"
      },
      "source": [
        "---\n",
        "## 8) 최종 비교: 세 모델의 번역 결과 비교\n",
        "\n",
        "이 섹션에서는 저장된 모델을 불러와서 같은 예시 문장에 대해 세 모델의 번역 결과를 비교합니다.\n",
        "\n",
        "### 비교 목적:\n",
        "1. **번역 품질**: 어떤 모델이 더 자연스럽고 정확한 번역을 생성하는가?\n",
        "2. **모델 특성**: 각 모델의 강점과 약점은 무엇인가?\n",
        "3. **실용성**: 실제 응용에서 어떤 모델을 선택해야 하는가?\n",
        "\n",
        "### 평가 관점:\n",
        "- 문법적 정확성\n",
        "- 어휘 선택의 적절성\n",
        "- 문장 구조의 자연스러움\n",
        "- 원문의 의미 보존"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhl0FN9Hphqn"
      },
      "outputs": [],
      "source": [
        "# ==================== 저장된 모델 로드 ====================\n",
        "\n",
        "# LSTM + Attention 모델 로드\n",
        "if os.path.exists(attn_ckpt):\n",
        "    model_attn.load_state_dict(torch.load(attn_ckpt, map_location=device))\n",
        "    print(f\"✓ LSTM Attention 모델 로드됨: {attn_ckpt}\")\n",
        "\n",
        "# From-Scratch Transformer 모델 로드\n",
        "scratch_ckpt_path = \"./checkpoints_enfr/from_scratch_transformer/model.pt\"\n",
        "if os.path.exists(scratch_ckpt_path):\n",
        "    model_scratch.load_state_dict(torch.load(scratch_ckpt_path, map_location=device))\n",
        "    print(f\"✓ From-Scratch Transformer 모델 로드됨: {scratch_ckpt_path}\")\n",
        "\n",
        "# Fine-tuned T5 모델 로드\n",
        "ft_model_path = \"./checkpoints_enfr/t5_finetuned\"\n",
        "if os.path.exists(ft_model_path):\n",
        "    model_ft = AutoModelForSeq2SeqLM.from_pretrained(ft_model_path).to(device)\n",
        "    print(f\"✓ Fine-tuned T5 모델 로드됨: {ft_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IMyKAEuphqn"
      },
      "outputs": [],
      "source": [
        "# 비교할 테스트 문장들 (다양한 난이도와 길이)\n",
        "comparison_sentences = [\n",
        "    \"This movie was surprisingly good and I would recommend it to everyone.\",\n",
        "    \"We will travel to Berlin next summer.\",\n",
        "    \"The weather is nice today, so we will go for a walk in the park.\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"최종 비교: LSTM+Attention vs From-Scratch Transformer vs Fine-tuned T5\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "# 각 문장에 대해 세 모델의 번역 비교\n",
        "for sent in comparison_sentences:\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(f\"EN (원문): {sent}\")\n",
        "    print(\"-\"*100)\n",
        "\n",
        "    # Model A: LSTM + Attention\n",
        "    attn_out, _, _ = translate_attn(model_attn, sent, collect_attn=False)\n",
        "    print(f\"DE [Model A: LSTM+Attention]:      {attn_out}\")\n",
        "\n",
        "    # Model B: From-scratch Transformer (BERT)\n",
        "    scratch_out = translate_transformer_scratch(model_scratch, sent)\n",
        "    print(f\"DE [Model B: From-Scratch Trans]:  {scratch_out[0]}\")\n",
        "\n",
        "    # Model C: Fine-tuned T5\n",
        "    ft_out = translate_t5(model_ft, [sent])\n",
        "    print(f\"DE [Model C: Fine-tuned T5]:       {ft_out[0]}\")\n",
        "\n",
        "    print(\"=\"*100)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pifi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}